{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-07 20:03:50.803601: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-01-07 20:03:50.803654: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(num_words=500)\n",
    "\n",
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "index2word = dict((i + 3, word) for (word, i) in word_index.items())\n",
    "index2word[0] = '[pad]'\n",
    "index2word[1] = '[bos]'\n",
    "index2word[2] = '[oov]'\n",
    "x_train = np.array([' '.join([index2word[idx] for idx in text]) for text in x_train])\n",
    "x_test = np.array([' '.join([index2word[idx] for idx in text]) for text in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list()\n",
    "for text in x_train:\n",
    "  tokens = text.split()\n",
    "  vocabulary.extend(tokens)\n",
    "\n",
    "vocabulary = set(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:39<00:00, 638.36it/s]\n",
      "100%|██████████| 25000/25000 [00:37<00:00, 660.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "x_train_binary = list()\n",
    "x_test_binary = list()\n",
    "\n",
    "for text in tqdm(x_train):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_train_binary.append(binary_vector)\n",
    "\n",
    "x_train_binary = np.array(x_train_binary)\n",
    "\n",
    "for text in tqdm(x_test):\n",
    "  tokens = text.split()\n",
    "  binary_vector = list()\n",
    "  for vocab_token in vocabulary:\n",
    "    if vocab_token in tokens:\n",
    "      binary_vector.append(1)\n",
    "    else:\n",
    "      binary_vector.append(0)\n",
    "  x_test_binary.append(binary_vector)\n",
    "\n",
    "x_test_binary = np.array(x_test_binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_positive = np.sum(y_train)/len(y_train)\n",
    "prior_negative = 1 - prior_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict():\n",
    "    predictions = list()\n",
    "    for idx1, correct in enumerate(y_test):\n",
    "        positive_prediction = prior_positive\n",
    "        negative_prediction = prior_negative\n",
    "        for idx2, word in enumerate(x_test_binary[idx1]):\n",
    "            if word:\n",
    "                positive_prediction *= positive_frequencies[idx2]\n",
    "                negative_prediction *= negative_frequencies[idx2]\n",
    "            else:\n",
    "                positive_prediction *= (1 - positive_frequencies[idx2])\n",
    "                negative_prediction *= (1 - negative_frequencies[idx2])\n",
    "\n",
    "        if positive_prediction >= negative_prediction:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1131., 1438.,  987.,  717.,  535.]),\n",
       " array([1510., 1497.,  951., 1085.,  420.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_count = np.zeros(len(x_train_binary[0]))\n",
    "negative_count = np.zeros(len(x_train_binary[0]))\n",
    "\n",
    "counter = 0\n",
    "for review, label in zip(x_train_binary, y_train):\n",
    "    for idx, word in enumerate(review):\n",
    "        if label == 1:\n",
    "            positive_count[idx] += word\n",
    "        else:\n",
    "            negative_count[idx] += word\n",
    "\n",
    "positive_count[:5], negative_count[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.09048, 0.11504, 0.07896, 0.05736, 0.0428 ]),\n",
       " array([0.1208 , 0.11976, 0.07608, 0.0868 , 0.0336 ]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_frequencies = np.array([x/(len(x_train_binary)*prior_positive) for x in positive_count])\n",
    "negative_frequencies = np.array([x/(len(x_train_binary)*prior_negative) for x in negative_count])\n",
    "\n",
    "positive_frequencies[:5], negative_frequencies[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = list()\n",
    "# for review, correct in zip(x_test_binary, y_test):\n",
    "#     positive_prediction = prior_positive\n",
    "#     negative_prediction = prior_negative\n",
    "\n",
    "#     present_indices = set(idx2 for idx2, word in enumerate(review) if word)\n",
    "#     for idx in range(max(len(positive_frequencies), len(negative_frequencies))):\n",
    "#         if idx in present_indices:\n",
    "#             positive_prediction *= positive_frequencies[idx]\n",
    "#             negative_prediction *= negative_frequencies[idx]\n",
    "#         else:\n",
    "#             positive_prediction *= (1 - positive_frequencies[idx])\n",
    "#             negative_prediction *= (1 - negative_frequencies[idx])\n",
    "\n",
    "#     if positive_prediction >= negative_prediction:\n",
    "#         predictions.append(1)\n",
    "#     else:\n",
    "#         predictions.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "nb = BernoulliNB()\n",
    "nb.fit(x_train_binary, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77     12500\n",
      "           1       0.76      0.82      0.79     12500\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.78      0.78      0.78     25000\n",
      "weighted avg       0.78      0.78      0.78     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, nb.predict(x_test_binary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.74      0.77     12500\n",
      "           1       0.76      0.82      0.79     12500\n",
      "\n",
      "    accuracy                           0.78     25000\n",
      "   macro avg       0.78      0.78      0.78     25000\n",
      "weighted avg       0.78      0.78      0.78     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predict()))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "62f3535457eb93f2f758cb7719215f0bdad15136930ff06358bb31ed0d687c07"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
